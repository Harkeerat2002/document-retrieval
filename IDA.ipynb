{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204d1025bd7a4e52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T19:13:54.677345Z",
     "start_time": "2024-11-05T19:13:53.325429Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from konlpy.tag import Okt\n",
    "import joblib\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.util import ngrams\n",
    "import gc\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "# Download nltk wordnet in /usr/lib/nltk_data\n",
    "nltk.download('wordnet')\n",
    "\n",
    "okt = Okt()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer_dict = {\n",
    "    'fr': SnowballStemmer('french'),\n",
    "    'de': SnowballStemmer('german'),\n",
    "    'es': SnowballStemmer('spanish'),\n",
    "    'it': SnowballStemmer('italian'),\n",
    "    'en': SnowballStemmer('english')\n",
    "}\n",
    "\n",
    "def load_stopwords(languages=['english', 'french', 'german', 'spanish', 'italian']):\n",
    "    stop_words = set()\n",
    "    for lang in languages:\n",
    "        stop_words.update(nltk.corpus.stopwords.words(lang))\n",
    "    return stop_words\n",
    "\n",
    "stop_words = load_stopwords(['english', 'french', 'german', 'spanish', 'italian'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c935481811db1c68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T19:13:54.773345Z",
     "start_time": "2024-11-05T19:13:54.678345Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize and 2-grams on 'fr', 'de', 'es', 'it'\n",
    "def preprocess_text(text, lang):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    if lang in ['en', 'fr', 'de', 'es', 'it']:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "    elif lang == 'ko':\n",
    "        tokens = okt.morphs(text)\n",
    "    else:\n",
    "        tokens = text.split()\n",
    "    \n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    if lang == 'en':\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    elif lang in ['fr', 'de', 'es', 'it']:\n",
    "        stemmer = stemmer_dict.get(lang, None)\n",
    "        if stemmer:\n",
    "            tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    if lang in ['fr', 'de', 'es', 'it'] and len(tokens) >= 2:\n",
    "        n_grams = ['_'.join(gram) for gram in ngrams(tokens, 2)]\n",
    "        tokens = tokens + n_grams\n",
    "    \n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "def split_into_batches(data, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    texts, langs = batch\n",
    "    return [preprocess_text(text, lang) for text, lang in zip(texts, langs)]\n",
    "\n",
    "def preprocess_texts(texts, langs, batch_size=10000):\n",
    "    batches = list(split_into_batches(list(zip(texts, langs)), batch_size))\n",
    "    \n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = list(tqdm(pool.imap(preprocess_batch, batches), total=len(batches), desc=\"Preprocess texts\"))\n",
    "    \n",
    "    # Flatten the list of lists\n",
    "    preprocessed_texts = [item for sublist in results for item in sublist]\n",
    "    return preprocessed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "893ebf24eff1fa01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T19:13:54.789345Z",
     "start_time": "2024-11-05T19:13:54.773345Z"
    }
   },
   "outputs": [],
   "source": [
    "# load and save\n",
    "def load_corpus(corpus_path='corpus.json'):\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "        corpus = json.load(f)\n",
    "    corpus_df = pd.DataFrame(corpus)\n",
    "    return corpus_df\n",
    "\n",
    "def load_data(train_path='train.csv', test_path='test.csv'):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    return train_df, test_df\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b76bc388c99da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T19:13:54.804567Z",
     "start_time": "2024-11-05T19:13:54.790347Z"
    }
   },
   "outputs": [],
   "source": [
    "# algorithm\n",
    "class BM25:\n",
    "    def __init__(self, tokenized_corpus, k1=1.5, b=0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.corpus_size = len(tokenized_corpus)\n",
    "        self.avgdl = sum(len(doc) for doc in tokenized_corpus) / self.corpus_size\n",
    "        self.df = defaultdict(int)\n",
    "        self.idf = {}\n",
    "        self.inverted_index = defaultdict(list)\n",
    "        self.term_freqs = []\n",
    "        self.build(tokenized_corpus)\n",
    "    \n",
    "    def build(self, tokenized_corpus):\n",
    "        for doc_id, document in enumerate(tqdm(tokenized_corpus, desc=\"Creating BM25 index\")):\n",
    "            freq = defaultdict(int)\n",
    "            for word in document:\n",
    "                freq[word] += 1\n",
    "            self.term_freqs.append(freq)\n",
    "            for word in freq.keys():\n",
    "                self.df[word] += 1\n",
    "                self.inverted_index[word].append(doc_id)\n",
    "        \n",
    "        for word, freq in self.df.items():\n",
    "            self.idf[word] = math.log(1 + (self.corpus_size - freq + 0.5) / (freq + 0.5))\n",
    "    \n",
    "    def get_scores(self, query):\n",
    "        scores = np.zeros(self.corpus_size)\n",
    "        unique_query_terms = set(query)\n",
    "        for word in unique_query_terms:\n",
    "            if word not in self.idf:\n",
    "                continue\n",
    "            idf = self.idf[word]\n",
    "            doc_ids = self.inverted_index[word]\n",
    "            for doc_id in doc_ids:\n",
    "                tf = self.term_freqs[doc_id][word]\n",
    "                dl = sum(self.term_freqs[doc_id].values())\n",
    "                score = idf * ((tf * (self.k1 + 1)) / (tf + self.k1 * (1 - self.b + dl / self.avgdl)))\n",
    "                scores[doc_id] += score\n",
    "        return scores\n",
    "    \n",
    "    def retrieve_top_n(self, query, n=10):\n",
    "        scores = self.get_scores(query)\n",
    "        if n >= len(scores):\n",
    "            top_n_indices = np.argsort(scores)[::-1]\n",
    "        else:\n",
    "            top_n_indices = np.argpartition(scores, -n)[-n:]\n",
    "            top_n_indices = top_n_indices[np.argsort(scores[top_n_indices])[::-1]]\n",
    "        return top_n_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d416b48dc98312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T19:32:19.442844Z",
     "start_time": "2024-11-05T19:13:54.805566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 19687, Val size: 2188\n",
      "Preprocessed corpus not found, generating ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocess texts:   0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "corpus_df = load_corpus('./data/corpus.json/corpus.json')\n",
    "train_df, test_df = load_data('./data/train.csv', './data/test.csv')\n",
    "\n",
    "# split\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}\")\n",
    "\n",
    "# generate or load preprocessed corpus\n",
    "preprocessed_corpus_path = './preprocessed_corpus.pkl'\n",
    "if os.path.exists(preprocessed_corpus_path):\n",
    "    print(f\"Loading exist preprocessed corpus: {preprocessed_corpus_path}\")\n",
    "    preprocessed_corpus = load_pickle(preprocessed_corpus_path)\n",
    "else:\n",
    "    print(f\"Preprocessed corpus not found, generating ...\")\n",
    "    document_texts = corpus_df['text'].tolist()\n",
    "    document_langs = corpus_df['lang'].tolist()\n",
    "    preprocessed_corpus = preprocess_texts(document_texts, document_langs)\n",
    "    save_pickle(preprocessed_corpus, preprocessed_corpus_path)\n",
    "    print(f\"Preprocessed corpus saved '{preprocessed_corpus_path}'。\")\n",
    "\n",
    "# process each lang\n",
    "lang_to_doc_indices = defaultdict(list)\n",
    "document_langs = corpus_df['lang'].tolist()\n",
    "document_ids = corpus_df['docid'].tolist()\n",
    "for idx, lang in enumerate(document_langs):\n",
    "    lang_to_doc_indices[lang].append(idx)\n",
    "\n",
    "bm25_models = {}\n",
    "doc_id_maps = {}\n",
    "\n",
    "for lang, indices in tqdm(lang_to_doc_indices.items(), desc=\"Processing languages\"):\n",
    "    print(f\"Processing '{lang}' ...\")\n",
    "    lang_texts = [preprocessed_corpus[idx] for idx in indices]\n",
    "    lang_docids = [document_ids[idx] for idx in indices]\n",
    "    \n",
    "    # token\n",
    "    tokenized_lang_path = f'./preprocessed_data/tokenized_{lang}.pkl'\n",
    "    if os.path.exists(tokenized_lang_path):\n",
    "        print(f\"token {tokenized_lang_path} already exit，skip generation\")\n",
    "        lang_tokenized_corpus = load_pickle(tokenized_lang_path)\n",
    "    else:\n",
    "        print(f\"Generating tokens for '{lang}' ...\")\n",
    "        if lang == 'ko':\n",
    "            lang_tokenized_corpus = [okt.morphs(text) for text in tqdm(lang_texts, desc=f\"tokenizaion {lang}\")]\n",
    "        else:\n",
    "            lang_tokenized_corpus = [text.split() for text in lang_texts]\n",
    "        save_pickle(lang_tokenized_corpus, tokenized_lang_path)\n",
    "        print(f\"Tokens saved to '{tokenized_lang_path}'。\")\n",
    "    \n",
    "    # model\n",
    "    bm25_model_path = f'bm25_model_{lang}.joblib'\n",
    "    if os.path.exists(bm25_model_path):\n",
    "        print(f\"BM25 model '{bm25_model_path}' already exist，skip generation\")\n",
    "        bm25_model = joblib.load(bm25_model_path)\n",
    "    else:\n",
    "        print(f\"Generating BM25 model for  '{lang}' ...\")\n",
    "        bm25_model = BM25(lang_tokenized_corpus, k1=1.5, b=0.75)\n",
    "        dump(model, f\"preprocessed_data/bm25_model_{lang}.joblib\", compress=3)\n",
    "        print(f\"BM25 model saved at '{bm25_model_path}'。\")\n",
    "    \n",
    "    bm25_models[lang] = bm25_model\n",
    "    doc_id_maps[lang] = lang_docids\n",
    "    \n",
    "    del lang_tokenized_corpus, bm25_model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f467dcf79899b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recall_at_k(bm25_models, doc_id_maps, val_data, k=10):\n",
    "    recall_count = 0\n",
    "    total = len(val_data)\n",
    "    for idx, row in val_data.iterrows():\n",
    "        query = row['query']\n",
    "        lang = row['lang']\n",
    "        pos_doc = row['positive_docs']\n",
    "        if lang not in bm25_models:\n",
    "            continue\n",
    "        bm25_model = bm25_models[lang]\n",
    "        doc_ids = doc_id_maps[lang]\n",
    "        \n",
    "        query_clean = preprocess_text(query, lang)\n",
    "        if lang == 'ko':\n",
    "            tokenized_query = okt.morphs(query_clean)\n",
    "        else:\n",
    "            tokenized_query = query_clean.split()\n",
    "        \n",
    "        top_n_indices = bm25_model.retrieve_top_n(tokenized_query, n=k)\n",
    "        retrieved_doc_ids = [doc_ids[i] for i in top_n_indices]\n",
    "        \n",
    "        if pos_doc in retrieved_doc_ids:\n",
    "            recall_count += 1\n",
    "    \n",
    "    recall = recall_count / total if total > 0 else 0\n",
    "    return recall\n",
    "\n",
    "print(\"Computing Val Recall@10...\")\n",
    "recall_at_10 = evaluate_recall_at_k(bm25_models, doc_id_maps, val_data, k=10)\n",
    "print(f\"Val Recall@10: {recall_at_10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458bd8ea5e606b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_test_queries(bm25_models, doc_id_maps, test_df, k=10):\n",
    "    retrieved_docs = []\n",
    "    for idx, row in test_df.iterrows():\n",
    "        query_id = row['query_id']\n",
    "        query = row['query']\n",
    "        lang = row['lang']\n",
    "        if lang not in bm25_models:\n",
    "            retrieved_docs.append([])\n",
    "            continue\n",
    "        bm25_model = bm25_models[lang]\n",
    "        doc_ids = doc_id_maps[lang]\n",
    "        \n",
    "        query_clean = preprocess_text(query, lang)\n",
    "        if lang == 'ko':\n",
    "            tokenized_query = okt.morphs(query_clean)\n",
    "        else:\n",
    "            tokenized_query = query_clean.split()\n",
    "        \n",
    "        top_n_indices = bm25_model.retrieve_top_n(tokenized_query, n=k)\n",
    "        retrieved_doc_ids = [doc_ids[i] for i in top_n_indices]\n",
    "        retrieved_docs.append(retrieved_doc_ids)\n",
    "    return retrieved_docs\n",
    "\n",
    "print(\"Retrive on test queries...\")\n",
    "retrieved_docs_test = retrieve_test_queries(bm25_models, doc_id_maps, test_df, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca22da07ae36618",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    'id': np.arange(len(test_df)),\n",
    "    'docids': retrieved_docs_test\n",
    "})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"'submission.csv' done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
