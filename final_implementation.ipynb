{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIS-Project-1: Document Retrieval\n",
    "\n",
    "This notebook contains the code for generating the final \"submission.csv\" file for the DIS Project 1. This code performs preprocessing on the queries and then retrieves the top 10 ranked documents from BM25 retrieval model. The final output is saved in the \"submission.csv\" file. This notebook does not generate the preprocessing of the corpus all the IDF TF calculations as they are loaded on it. You can fine all the code with full implementation on https://github.com/JiayiLi1608/DIS_LJY_HS_AS Github Repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Intital Setup and Data Loading\n",
    "\n",
    "This section contains the code for loading the data and setting up the Kaggle environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "All the Libraries used in the code are imported in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "import nltk\n",
    "from konlpy.tag import Okt\n",
    "from nltk.stem import SnowballStemmer\n",
    "import tqdm\n",
    "import string\n",
    "from nltk.util import ngrams\n",
    "from joblib import dump\n",
    "import time\n",
    "from joblib import dump, load\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "! unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/ # Needed because in kaggle the wordnet is not unzipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Bellow is the code for preprocessing the queries. The preprocessing steps include:\n",
    "- Lowercasing\n",
    "- Removing Punctuation\n",
    "- Removing Stopwords\n",
    "- Tokenization\n",
    "- Lemmatization\n",
    "\n",
    "Preprocessing is done a bit differently per language wise. Especially for the Korean language as it requires a different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords\n",
    "def load_stopwords(languages=['english', 'french', 'german', 'spanish', 'italian']):\n",
    "    stop_words = set()\n",
    "    for lang in languages:\n",
    "        try:\n",
    "            stop_words.update(nltk.corpus.stopwords.words(lang))\n",
    "        except:\n",
    "            pass\n",
    "    return stop_words\n",
    "\n",
    "def preprocess_text(text, lang):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    if lang in ['en', 'fr', 'de', 'es', 'it']:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "    elif lang == 'ko':\n",
    "        tokens = okt.morphs(text)\n",
    "    else:\n",
    "        tokens = text.split()\n",
    "    \n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    if lang == 'en':\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    elif lang in ['fr', 'de', 'es', 'it']:\n",
    "        stemmer = stemmer_dict.get(lang, None)\n",
    "        if stemmer:\n",
    "            tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    if lang in ['fr', 'de', 'es', 'it'] and len(tokens) >= 2:\n",
    "        n_grams = ['_'.join(gram) for gram in ngrams(tokens, 2)]\n",
    "        tokens = tokens + n_grams\n",
    "    \n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "#stemmers and tokenizers\n",
    "okt = Okt()\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "stemmer_dict = {\n",
    "    'fr': SnowballStemmer('french'),\n",
    "    'de': SnowballStemmer('german'),\n",
    "    'es': SnowballStemmer('spanish'),\n",
    "    'it': SnowballStemmer('italian'),\n",
    "    'en': SnowballStemmer('english')\n",
    "}\n",
    "\n",
    "print(\"Loading stopwords...\")\n",
    "stop_words = load_stopwords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 Retrieval Model\n",
    "Bellow is the class for the BM25 retrieval model. The class contains the following methods:\n",
    "- `__init__`: Initializes the BM25 model with the given parameters.\n",
    "- `build`: Builds the term-frequency and document frequency dictionaries, and the inverted index. \n",
    "- `precompute_idf`: Precomputes the IDF values for all the terms in the corpus.\n",
    "- `calculate_scores`: Calculates the BM25 scores for all the documents in the corpus.\n",
    "- `retrieve_top_n`: Returns the top N ranked documents for a given query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25:\n",
    "    def __init__(self, k1=1.5, b=0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.corpus_size = 0.0\n",
    "        self.avgdl = 0.0\n",
    "        self.df = defaultdict(int)\n",
    "        self.idf = {}\n",
    "        self.inverted_index = defaultdict(list)\n",
    "        self.term_freqs = []\n",
    "        self.doc_lengths = 0\n",
    "        self.precomputed_idf = 0\n",
    "\n",
    "    def build(self, tokenized_corpus, lang):\n",
    "        for doc_id, document in enumerate(tokenized_corpus):\n",
    "            freq = defaultdict(int)\n",
    "            for word in document:\n",
    "                freq[word] += 1\n",
    "            self.term_freqs.append(freq)\n",
    "            for word in freq.keys():\n",
    "                self.df[word] += 1\n",
    "                self.inverted_index[word].append(doc_id)\n",
    "\n",
    "        for word, freq in self.df.items():\n",
    "            self.idf[word] = math.log(\n",
    "                1 + (self.corpus_size - freq + 0.5) / (freq + 0.5)\n",
    "            )\n",
    "    def precompute_doc_lengths(self):\n",
    "        return {doc_id: sum(self.term_freqs[doc_id].values()) for doc_id in tqdm.tqdm(range(self.corpus_size))}\n",
    "\n",
    "    def precompute_idf(self):\n",
    "        return {word: self.idf[word] for word in tqdm.tqdm(self.idf.keys())}\n",
    "\n",
    "    def calculate_scores(self, query):\n",
    "        scores = np.zeros(self.corpus_size)\n",
    "        unique_query_terms = set(query)\n",
    "        k1 = self.k1\n",
    "        b = self.b\n",
    "        avgdl = self.avgdl\n",
    "\n",
    "        for word in unique_query_terms:\n",
    "            if word not in self.precomputed_idf:\n",
    "                continue\n",
    "            idf = self.precomputed_idf[word]\n",
    "            doc_ids = self.inverted_index[word]\n",
    "            for doc_id in doc_ids:\n",
    "                tf = self.term_freqs[doc_id][word]\n",
    "                dl = self.doc_lengths[doc_id]\n",
    "                score = idf * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + dl / avgdl)))\n",
    "                scores[doc_id] += score\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def retrieve_top_n(self, query, n=10):\n",
    "        scores = self.calculate_scores(query)\n",
    "        if n >= len(scores):\n",
    "            top_n_indices = np.argsort(scores)[::-1]\n",
    "        else:\n",
    "            top_n_indices = np.argpartition(scores, -n)[-n:]\n",
    "            top_n_indices = top_n_indices[np.argsort(scores[top_n_indices])[::-1]]\n",
    "        return top_n_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search Class\n",
    "The main purpose of this class is to load and preprocess the search queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "class HybridSearch:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_preprocessed_queries(self):\n",
    "        print(\"Preprocessing the Queries\")\n",
    "        queries_path = f'/kaggle/input/dis-project-1/preprocessed_test_queries.pkl'\n",
    "        langs_path = f'/kaggle/input/dis-project-1/test_query_langs.pkl'\n",
    "        test_path = f'/kaggle/input/dis-project-1-document-retrieval/test.csv'\n",
    "        if os.path.exists(langs_path):\n",
    "            #preprocessed_queries = load_pickle(queries_path)\n",
    "            query_langs = load_pickle(langs_path)\n",
    "        print(\"Starting Preprocessing\")\n",
    "        test_cv = pd.read_csv(test_path)\n",
    "        preprocessed_q = []\n",
    "        for query, lang in tqdm.tqdm(zip(test_cv['query'], test_cv['lang'])):\n",
    "            preprocessed_q.append(preprocess_text(query, lang))\n",
    "\n",
    "        return preprocessed_q, query_langs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Function for Loading Preprocessed Data\n",
    "Bellow are the functions mainly aimed to load the preprocessed data very quickly. Due the limitations of Kaggle I/O speed, it takes a lot of time to load the data. For his reason we had to put all the data into batches, and then load them up again in batches while in parallel processing. This way we reduced a lot of time in loading the data. However still it takes a lot of time to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_n_batch(args):\n",
    "    bm25_model, tokenized_query_batch, k = args\n",
    "    return [bm25_model.retrieve_top_n(query, n=k) for query in tqdm.tqdm(tokenized_query_batch)]\n",
    "\n",
    "def save_model_compressed(model, path):\n",
    "    # Save the model with compression\n",
    "    dump(model, path)\n",
    "\n",
    "def load_model_mmap(path):\n",
    "    # Load the model with memory mapping\n",
    "    return load(path, mmap_mode='r+')\n",
    "\n",
    "def load_pickle_file(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_model_picklebatches(path, lang, batch_size=10):\n",
    "    batch_per_lang = {\"ar\": 9, \"de\": 11, \"en\": 208, \"es\": 12, \"fr\": 11, \"it\": 12, \"ko\": 8}\n",
    "    \n",
    "    if lang == \"en\":\n",
    "        batch_size = 10\n",
    "    else:\n",
    "        batch_size = 5\n",
    "    \n",
    "    term_freqs = []\n",
    "    file_paths = [f\"{path}/batch_{i}_{lang}.pkl\" for i in range(batch_per_lang[lang])]\n",
    "    \n",
    "    # Split file paths into smaller batches\n",
    "    file_batches = [file_paths[i:i + batch_size] for i in range(0, len(file_paths), batch_size)]\n",
    "    \n",
    "    for file_batch in tqdm.tqdm(file_batches):\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(load_pickle_file, file_batch))\n",
    "        \n",
    "        for result in results:\n",
    "            term_freqs.extend(result)\n",
    "    \n",
    "    return term_freqs\n",
    "\n",
    "def load_model_picklebatches_idf(path, lang, batch_size=3):\n",
    "    batch_per_lang = {\"ar\": 2, \"de\": 19, \"en\": 6, \"es\": 14, \"fr\": 16, \"it\": 17, \"ko\": 1}\n",
    "    \n",
    "    \n",
    "    idf = {}\n",
    "    file_paths = [f\"{path}/batch_idf{i}_{lang}.pkl\" for i in range(batch_per_lang[lang])]\n",
    "    \n",
    "    # Split file paths into smaller batches\n",
    "    file_batches = [file_paths[i:i + batch_size] for i in range(0, len(file_paths), batch_size)]\n",
    "    \n",
    "    for file_batch in tqdm.tqdm(file_batches, desc=f\"Loading {lang} IDF\"):\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(load_pickle_file, file_batch))\n",
    "        \n",
    "        for result in results:\n",
    "            idf.update(result)\n",
    "            \n",
    "    return idf\n",
    "\n",
    "\n",
    "def load_model_picklebatches_id(path, lang, batch_size=3):\n",
    "    batch_per_lang = {\"ar\": 2, \"de\": 19, \"en\": 6, \"es\": 14, \"fr\": 16, \"it\": 17, \"ko\": 1}\n",
    "    \n",
    "    inverted_index = defaultdict(list)\n",
    "    file_paths = [f\"{path}/batch_inverse{i}_{lang}.pkl\" for i in range(batch_per_lang[lang])]\n",
    "    \n",
    "    # Split file paths into smaller batches\n",
    "    file_batches = [file_paths[i:i + batch_size] for i in range(0, len(file_paths), batch_size)]\n",
    "    \n",
    "    for file_batch in tqdm.tqdm(file_batches, desc=f\"Loading {lang} inverted index\"):\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(load_pickle_file, file_batch))\n",
    "        \n",
    "        for result in results:\n",
    "            for key, value in result.items():\n",
    "                inverted_index[key].extend(value)\n",
    "                \n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Ranking the Documents\n",
    "\n",
    " To make the ranking of the documents much faster, we have used batches to sequently compute the BM25 scores for the documents. This way we aimed to reduce the time of computation by a lot. The parameters values of Batch Size, k1, b, and n are set to the optimal values. The optimal values were found by running the model ,multiple times and then selecting the best values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_test_queries_optimized(preprocessed_queries, query_langs, k=10):\n",
    "    retrieved_docs = [None] * len(preprocessed_queries)\n",
    "    queries_df = pd.DataFrame(\n",
    "        {\n",
    "            \"query\": preprocessed_queries,\n",
    "            \"lang\": query_langs,\n",
    "            \"original_idx\": range(len(preprocessed_queries)),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Average document lengths\n",
    "    avgdl_dict = {\n",
    "        \"ar\": 4418.960584437648,\n",
    "        \"de\": 5575.8404294032025,\n",
    "        \"en\": 1339.3939950714448,\n",
    "        \"es\": 6174.31717941737,\n",
    "        \"fr\": 6834.264518546272,\n",
    "        \"it\": 6483.073170731707,\n",
    "        \"ko\": 4380.481946028126\n",
    "    }\n",
    "    \n",
    "    # Corpus sizes\n",
    "    corpus_size_dict = {\n",
    "        \"ar\": 8829,\n",
    "        \"de\": 10992,\n",
    "        \"en\": 207363,\n",
    "        \"es\": 11019,\n",
    "        \"fr\": 10676,\n",
    "        \"it\": 11250,\n",
    "        \"ko\": 7893\n",
    "    }\n",
    "    \n",
    "    grouped = queries_df.groupby(\"lang\")\n",
    "\n",
    "    for lang, group in tqdm.tqdm(grouped):\n",
    "        \n",
    "        # Paths to load the model\n",
    "        lang_queries = group[\"query\"].tolist()\n",
    "        lang_original_indices = group[\"original_idx\"].tolist()\n",
    "        lang_doc_ids_path = f\"/kaggle/input/dis-project-1/doc_ids_{lang}.pkl\"\n",
    "        lang_bm25_model_path = f\"/kaggle/input/dis-project-1/bm25_model_{lang}.pkl\"\n",
    "        lang_bm25_model_path_joblib = \"/kaggle/input/dis-project-1/\"\n",
    "            \n",
    "        \n",
    "\n",
    "        # Load model with memory mapping\n",
    "        print(f\"Loading {lang_bm25_model_path}\")\n",
    "        \n",
    "        term_freqs = load_model_picklebatches(lang_bm25_model_path_joblib, lang)\n",
    "\n",
    "        # Loading the rest of the model\n",
    "        start_time = time.time()\n",
    "        idf = load_model_picklebatches_idf(\"/kaggle/input/dis-project-1\", lang)\n",
    "        end_time = time.time() - start_time\n",
    "        print(f\"IDF load time: {end_time}\")\n",
    "        start_time = time.time()\n",
    "        inverted_index = load_model_picklebatches_id(\"/kaggle/input/dis-project-1/\", lang)\n",
    "        end_time = time.time() - start_time\n",
    "        print(f\"Inverted Index load time: {end_time}\")\n",
    "            \n",
    "        start_time = time.time()\n",
    "        bm25_model = BM25()\n",
    "        bm25_model.term_freqs = term_freqs\n",
    "        bm25_model.corpus_size = corpus_size_dict[lang]\n",
    "        bm25_model.avgdl = avgdl_dict[lang]\n",
    "        bm25_model.idf = idf\n",
    "        bm25_model.inverted_index = inverted_index\n",
    "        bm25_model.doc_lengths = bm25_model.precompute_doc_lengths()\n",
    "        bm25_model.precomputed_idf = bm25_model.precompute_idf()\n",
    "        end_time = time.time() - start_time\n",
    "        print(f\"BM25 model load time: {end_time}\")\n",
    "\n",
    "        # Load document IDs with memory mapping\n",
    "        with open(lang_doc_ids_path, \"rb\") as f:\n",
    "            doc_ids = pickle.load(f)\n",
    "\n",
    "        batch_size = 100\n",
    "        if lang == \"en\":\n",
    "            batch_size = 200\n",
    "        tokenized_queries = [query.split() for query in lang_queries]\n",
    "        num_batches = math.ceil(len(tokenized_queries) / batch_size)\n",
    "        batches = [tokenized_queries[i * batch_size:(i + 1) * batch_size] for i in range(num_batches)]\n",
    "        original_idx_batches = [lang_original_indices[i * batch_size:(i + 1) * batch_size] for i in range(num_batches)]\n",
    "\n",
    "        # Retrieve top-k documents for each batch of queries\n",
    "        print(f\"Retrieving top-{k} documents for {lang} queries...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Process batches sequentially\n",
    "        results = []\n",
    "        for batch in batches:\n",
    "            result = retrieve_top_n_batch((bm25_model, batch, k))\n",
    "            results.append(result)\n",
    "\n",
    "        retrieval_time = time.time() - start_time\n",
    "        print(f\"Retrieval time: {retrieval_time}\")\n",
    "        \n",
    "        # Assign retrieved documents to original indices\n",
    "        for batch_results, original_indices in zip(results, original_idx_batches):\n",
    "            for i, result in enumerate(batch_results):\n",
    "                retrieved_docs[original_indices[i]] = [doc_ids[idx] for idx in result]\n",
    "\n",
    "        del bm25_model, doc_ids, lang_queries, tokenized_queries\n",
    "        gc.collect()\n",
    "        \n",
    "    return retrieved_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "The bellow function is the main function that is used to generate the final submission file. The function loads the preprocessed data, and then retrieves the top 10 ranked documents for each query. The final output is saved in the \"submission.csv\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "        \n",
    "    #Load test data and preprocessed queries\n",
    "    preprocessed_test_queries, test_query_langs = HybridSearch().load_preprocessed_queries()\n",
    "\n",
    "    print(preprocessed_test_queries[:5])\n",
    "    print(test_query_langs[:5])\n",
    "    #Perform retrieval on test set\n",
    "    retrieved_docs_test = retrieve_test_queries_optimized(\n",
    "        preprocessed_queries=preprocessed_test_queries,\n",
    "        query_langs=test_query_langs,\n",
    "        k=10\n",
    "    )\n",
    "\n",
    "    print(retrieved_docs_test[:5])\n",
    "\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': np.arange(len(retrieved_docs_test)),\n",
    "        'docids': retrieved_docs_test\n",
    "    })\n",
    "\n",
    "    print(submission_df[:5])\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
