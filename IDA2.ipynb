{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204d1025bd7a4e52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T19:13:54.677345Z",
     "start_time": "2024-11-05T19:13:53.325429Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from konlpy.tag import Okt\n",
    "import joblib\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.util import ngrams\n",
    "import gc\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "okt = Okt()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer_dict = {\n",
    "    'fr': SnowballStemmer('french'),\n",
    "    'de': SnowballStemmer('german'),\n",
    "    'es': SnowballStemmer('spanish'),\n",
    "    'it': SnowballStemmer('italian'),\n",
    "    'en': SnowballStemmer('english')\n",
    "}\n",
    "\n",
    "def load_stopwords(languages=['english', 'french', 'german', 'spanish', 'italian']):\n",
    "    stop_words = set()\n",
    "    for lang in languages:\n",
    "        stop_words.update(nltk.corpus.stopwords.words(lang))\n",
    "    return stop_words\n",
    "\n",
    "stop_words = load_stopwords(['english', 'french', 'german', 'spanish', 'italian'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c935481811db1c68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T19:13:54.773345Z",
     "start_time": "2024-11-05T19:13:54.678345Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize and 2-grams on 'fr', 'de', 'es', 'it'\n",
    "def preprocess_text(text, lang):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    if lang in ['en', 'fr', 'de', 'es', 'it']:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "    elif lang == 'ko':\n",
    "        tokens = okt.morphs(text)\n",
    "    else:\n",
    "        tokens = text.split()\n",
    "    \n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    if lang == 'en':\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    elif lang in ['fr', 'de', 'es', 'it']:\n",
    "        stemmer = stemmer_dict.get(lang, None)\n",
    "        if stemmer:\n",
    "            tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    if lang in ['fr', 'de', 'es', 'it'] and len(tokens) >= 2:\n",
    "        n_grams = ['_'.join(gram) for gram in ngrams(tokens, 2)]\n",
    "        tokens = tokens + n_grams\n",
    "    \n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "def preprocess_texts(texts, langs):\n",
    "    preprocessed_texts = []\n",
    "    for text, lang in tqdm(zip(texts, langs), total=len(texts), desc=\"Preprocess texts\"):\n",
    "        cleaned = preprocess_text(text, lang)\n",
    "        preprocessed_texts.append(cleaned)\n",
    "    return preprocessed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "893ebf24eff1fa01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T19:13:54.789345Z",
     "start_time": "2024-11-05T19:13:54.773345Z"
    }
   },
   "outputs": [],
   "source": [
    "# load and save\n",
    "def load_corpus(corpus_path='corpus.json'):\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "        corpus = json.load(f)\n",
    "    corpus_df = pd.DataFrame(corpus)\n",
    "    return corpus_df\n",
    "\n",
    "def load_data(train_path='train.csv', test_path='test.csv'):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    return train_df, test_df\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b76bc388c99da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T19:13:54.804567Z",
     "start_time": "2024-11-05T19:13:54.790347Z"
    }
   },
   "outputs": [],
   "source": [
    "# algorithm\n",
    "class BM25:\n",
    "    def __init__(self, tokenized_corpus, k1=1.5, b=0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.corpus_size = len(tokenized_corpus)\n",
    "        self.avgdl = sum(len(doc) for doc in tokenized_corpus) / self.corpus_size\n",
    "        self.df = defaultdict(int)\n",
    "        self.idf = {}\n",
    "        self.inverted_index = defaultdict(list)\n",
    "        self.term_freqs = []\n",
    "        self.build(tokenized_corpus)\n",
    "    \n",
    "    def build(self, tokenized_corpus):\n",
    "        for doc_id, document in enumerate(tqdm(tokenized_corpus, desc=\"Creating BM25 index\")):\n",
    "            freq = defaultdict(int)\n",
    "            for word in document:\n",
    "                freq[word] += 1\n",
    "            self.term_freqs.append(freq)\n",
    "            for word in freq.keys():\n",
    "                self.df[word] += 1\n",
    "                self.inverted_index[word].append(doc_id)\n",
    "        \n",
    "        for word, freq in self.df.items():\n",
    "            self.idf[word] = math.log(1 + (self.corpus_size - freq + 0.5) / (freq + 0.5))\n",
    "    \n",
    "    def get_scores(self, query):\n",
    "        scores = np.zeros(self.corpus_size)\n",
    "        unique_query_terms = set(query)\n",
    "        for word in unique_query_terms:\n",
    "            if word not in self.idf:\n",
    "                continue\n",
    "            idf = self.idf[word]\n",
    "            doc_ids = self.inverted_index[word]\n",
    "            for doc_id in doc_ids:\n",
    "                tf = self.term_freqs[doc_id][word]\n",
    "                dl = sum(self.term_freqs[doc_id].values())\n",
    "                score = idf * ((tf * (self.k1 + 1)) / (tf + self.k1 * (1 - self.b + dl / self.avgdl)))\n",
    "                scores[doc_id] += score\n",
    "        return scores\n",
    "    \n",
    "    def retrieve_top_n(self, query, n=10):\n",
    "        scores = self.get_scores(query)\n",
    "        if n >= len(scores):\n",
    "            top_n_indices = np.argsort(scores)[::-1]\n",
    "        else:\n",
    "            top_n_indices = np.argpartition(scores, -n)[-n:]\n",
    "            top_n_indices = top_n_indices[np.argsort(scores[top_n_indices])[::-1]]\n",
    "        return top_n_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94d416b48dc98312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T19:32:19.442844Z",
     "start_time": "2024-11-05T19:13:54.805566Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# split\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m corpus_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/corpus.json/corpus.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m train_df, test_df \u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m train_data, val_data \u001b[38;5;241m=\u001b[39m train_test_split(train_df, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m, in \u001b[0;36mload_corpus\u001b[1;34m(corpus_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_corpus\u001b[39m(corpus_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorpus.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(corpus_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 4\u001b[0m         corpus \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     corpus_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(corpus)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m corpus_df\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\dis-project\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m<frozen codecs>:319\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# split\n",
    "corpus_df = load_corpus('./data/corpus.json/corpus.json')\n",
    "train_df, test_df = load_data('./data/train.csv', './data/test.csv')\n",
    "\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}\")\n",
    "\n",
    "# generate or load preprocessed corpus\n",
    "preprocessed_corpus_path = 'preprocessed_corpus.pkl'\n",
    "if os.path.exists(preprocessed_corpus_path):\n",
    "    print(f\"Loading exist preprocessed corpus: {preprocessed_corpus_path}\")\n",
    "    preprocessed_corpus = load_pickle(preprocessed_corpus_path)\n",
    "else:\n",
    "    document_texts = corpus_df['text'].tolist()\n",
    "    document_langs = corpus_df['lang'].tolist()\n",
    "    preprocessed_corpus = preprocess_texts(document_texts, document_langs)\n",
    "    save_pickle(preprocessed_corpus, preprocessed_corpus_path)\n",
    "    print(f\"Preprocessed corpus saved '{preprocessed_corpus_path}'。\")\n",
    "\n",
    "# process each lang\n",
    "lang_to_doc_indices = defaultdict(list)\n",
    "document_langs = corpus_df['lang'].tolist()\n",
    "document_ids = corpus_df['docid'].tolist()\n",
    "for idx, lang in enumerate(document_langs):\n",
    "    lang_to_doc_indices[lang].append(idx)\n",
    "\n",
    "bm25_models = {}\n",
    "doc_id_maps = {}\n",
    "\n",
    "def process_lang(lang, indices):\n",
    "    print(f\"Processing '{lang}' ...\")\n",
    "    lang_texts = [preprocessed_corpus[idx] for idx in indices]\n",
    "    lang_docids = [document_ids[idx] for idx in indices]\n",
    "    \n",
    "    # token\n",
    "    tokenized_lang_path = f'tokenized_{lang}.pkl'\n",
    "    if os.path.exists(tokenized_lang_path):\n",
    "        print(f\"token {tokenized_lang_path} already exsit，skip generation\")\n",
    "        lang_tokenized_corpus = load_pickle(tokenized_lang_path)\n",
    "    else:\n",
    "        print(f\"Generating tokens for '{lang}' ...\")\n",
    "        if lang == 'ko':\n",
    "            lang_tokenized_corpus = [okt.morphs(text) for text in tqdm(lang_texts, desc=f\"tokenizaion {lang}\")]\n",
    "        else:\n",
    "            lang_tokenized_corpus = [text.split() for text in lang_texts]\n",
    "        save_pickle(lang_tokenized_corpus, tokenized_lang_path)\n",
    "        print(f\"Tokens saved to '{tokenized_lang_path}'。\")\n",
    "    \n",
    "    # model\n",
    "    # Add your model processing code here\n",
    "\n",
    "    return lang, lang_tokenized_corpus\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    results = list(executor.map(lambda item: process_lang(*item), lang_to_doc_indices.items()))\n",
    "\n",
    "# Collect results\n",
    "for lang, lang_tokenized_corpus in results:\n",
    "    bm25_models[lang] = lang_tokenized_corpus  # Replace with actual model\n",
    "    doc_id_maps[lang] = lang_tokenized_corpus  # Replace with actual doc_id_map\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f467dcf79899b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recall_at_k(bm25_models, doc_id_maps, val_data, k=10):\n",
    "    recall_count = 0\n",
    "    total = len(val_data)\n",
    "    for idx, row in val_data.iterrows():\n",
    "        query = row['query']\n",
    "        lang = row['lang']\n",
    "        pos_doc = row['positive_docs']\n",
    "        if lang not in bm25_models:\n",
    "            continue\n",
    "        bm25_model = bm25_models[lang]\n",
    "        doc_ids = doc_id_maps[lang]\n",
    "        \n",
    "        query_clean = preprocess_text(query, lang)\n",
    "        if lang == 'ko':\n",
    "            tokenized_query = okt.morphs(query_clean)\n",
    "        else:\n",
    "            tokenized_query = query_clean.split()\n",
    "        \n",
    "        top_n_indices = bm25_model.retrieve_top_n(tokenized_query, n=k)\n",
    "        retrieved_doc_ids = [doc_ids[i] for i in top_n_indices]\n",
    "        \n",
    "        if pos_doc in retrieved_doc_ids:\n",
    "            recall_count += 1\n",
    "    \n",
    "    recall = recall_count / total if total > 0 else 0\n",
    "    return recall\n",
    "\n",
    "print(\"Computing Val Recall@10...\")\n",
    "recall_at_10 = evaluate_recall_at_k(bm25_models, doc_id_maps, val_data, k=10)\n",
    "print(f\"Val Recall@10: {recall_at_10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458bd8ea5e606b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def retrieve_test_queries(bm25_models, doc_id_maps, test_df, k=10, batch_size=100):\n",
    "    def process_batch(batch):\n",
    "        results = []\n",
    "        for _, row in batch.iterrows():\n",
    "            query_id = row['query_id']\n",
    "            query = row['query']\n",
    "            lang = row['lang']\n",
    "            if lang not in bm25_models:\n",
    "                results.append([])\n",
    "                continue\n",
    "            bm25_model = bm25_models[lang]\n",
    "            doc_ids = doc_id_maps[lang]\n",
    "\n",
    "            query_clean = preprocess_text(query, lang)\n",
    "            if lang == 'ko':\n",
    "                tokenized_query = okt.morphs(query_clean)\n",
    "            else:\n",
    "                tokenized_query = query_clean.split()\n",
    "\n",
    "            top_n_indices = bm25_model.retrieve_top_n(tokenized_query, n=k)\n",
    "            retrieved_doc_ids = [doc_ids[i] for i in top_n_indices]\n",
    "            results.append(retrieved_doc_ids)\n",
    "        return results\n",
    "\n",
    "    # Split the DataFrame into batches\n",
    "    batches = np.array_split(test_df, np.ceil(len(test_df) / batch_size))\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        retrieved_docs = list(executor.map(process_batch, batches))\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    retrieved_docs = [doc for batch in retrieved_docs for doc in batch]\n",
    "    \n",
    "    return retrieved_docs\n",
    "\n",
    "print(\"Retrive on test queries...\")\n",
    "retrieved_docs_test = retrieve_test_queries(bm25_models, doc_id_maps, test_df, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca22da07ae36618",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    'id': np.arange(len(test_df)),\n",
    "    'docids': retrieved_docs_test\n",
    "})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"'submission.csv' done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
