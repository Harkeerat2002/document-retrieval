{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "204d1025bd7a4e52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T19:13:54.677345Z",
     "start_time": "2024-11-05T19:13:53.325429Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jiayi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jiayi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jiayi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from konlpy.tag import Okt\n",
    "import joblib\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.util import ngrams\n",
    "import gc\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "okt = Okt()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer_dict = {\n",
    "    'fr': SnowballStemmer('french'),\n",
    "    'de': SnowballStemmer('german'),\n",
    "    'es': SnowballStemmer('spanish'),\n",
    "    'it': SnowballStemmer('italian'),\n",
    "    'en': SnowballStemmer('english')\n",
    "}\n",
    "\n",
    "def load_stopwords(languages=['english', 'french', 'german', 'spanish', 'italian']):\n",
    "    stop_words = set()\n",
    "    for lang in languages:\n",
    "        stop_words.update(nltk.corpus.stopwords.words(lang))\n",
    "    return stop_words\n",
    "\n",
    "stop_words = load_stopwords(['english', 'french', 'german', 'spanish', 'italian'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c935481811db1c68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T19:13:54.773345Z",
     "start_time": "2024-11-05T19:13:54.678345Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize and 2-grams on 'fr', 'de', 'es', 'it'\n",
    "def preprocess_text(text, lang):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    if lang in ['en', 'fr', 'de', 'es', 'it']:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "    elif lang == 'ko':\n",
    "        tokens = okt.morphs(text)\n",
    "    else:\n",
    "        tokens = text.split()\n",
    "    \n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    if lang == 'en':\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    elif lang in ['fr', 'de', 'es', 'it']:\n",
    "        stemmer = stemmer_dict.get(lang, None)\n",
    "        if stemmer:\n",
    "            tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    if lang in ['fr', 'de', 'es', 'it'] and len(tokens) >= 2:\n",
    "        n_grams = ['_'.join(gram) for gram in ngrams(tokens, 2)]\n",
    "        tokens = tokens + n_grams\n",
    "    \n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "def preprocess_texts(texts, langs):\n",
    "    preprocessed_texts = []\n",
    "    for text, lang in tqdm(zip(texts, langs), total=len(texts), desc=\"Preprocess texts\"):\n",
    "        cleaned = preprocess_text(text, lang)\n",
    "        preprocessed_texts.append(cleaned)\n",
    "    return preprocessed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "893ebf24eff1fa01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T19:13:54.789345Z",
     "start_time": "2024-11-05T19:13:54.773345Z"
    }
   },
   "outputs": [],
   "source": [
    "# load and save\n",
    "def load_corpus(corpus_path='corpus.json'):\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "        corpus = json.load(f)\n",
    "    corpus_df = pd.DataFrame(corpus)\n",
    "    return corpus_df\n",
    "\n",
    "def load_data(train_path='train.csv', test_path='test.csv'):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    return train_df, test_df\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b76bc388c99da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T19:13:54.804567Z",
     "start_time": "2024-11-05T19:13:54.790347Z"
    }
   },
   "outputs": [],
   "source": [
    "# algorithm\n",
    "class BM25:\n",
    "    def __init__(self, tokenized_corpus, k1=1.5, b=0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.corpus_size = len(tokenized_corpus)\n",
    "        self.avgdl = sum(len(doc) for doc in tokenized_corpus) / self.corpus_size\n",
    "        self.df = defaultdict(int)\n",
    "        self.idf = {}\n",
    "        self.inverted_index = defaultdict(list)\n",
    "        self.term_freqs = []\n",
    "        self.build(tokenized_corpus)\n",
    "    \n",
    "    def build(self, tokenized_corpus):\n",
    "        for doc_id, document in enumerate(tqdm(tokenized_corpus, desc=\"Creating BM25 index\")):\n",
    "            freq = defaultdict(int)\n",
    "            for word in document:\n",
    "                freq[word] += 1\n",
    "            self.term_freqs.append(freq)\n",
    "            for word in freq.keys():\n",
    "                self.df[word] += 1\n",
    "                self.inverted_index[word].append(doc_id)\n",
    "        \n",
    "        for word, freq in self.df.items():\n",
    "            self.idf[word] = math.log(1 + (self.corpus_size - freq + 0.5) / (freq + 0.5))\n",
    "    \n",
    "    def get_scores(self, query):\n",
    "        scores = np.zeros(self.corpus_size)\n",
    "        unique_query_terms = set(query)\n",
    "        for word in unique_query_terms:\n",
    "            if word not in self.idf:\n",
    "                continue\n",
    "            idf = self.idf[word]\n",
    "            doc_ids = self.inverted_index[word]\n",
    "            for doc_id in doc_ids:\n",
    "                tf = self.term_freqs[doc_id][word]\n",
    "                dl = sum(self.term_freqs[doc_id].values())\n",
    "                score = idf * ((tf * (self.k1 + 1)) / (tf + self.k1 * (1 - self.b + dl / self.avgdl)))\n",
    "                scores[doc_id] += score\n",
    "        return scores\n",
    "    \n",
    "    def retrieve_top_n(self, query, n=10):\n",
    "        scores = self.get_scores(query)\n",
    "        if n >= len(scores):\n",
    "            top_n_indices = np.argsort(scores)[::-1]\n",
    "        else:\n",
    "            top_n_indices = np.argpartition(scores, -n)[-n:]\n",
    "            top_n_indices = top_n_indices[np.argsort(scores[top_n_indices])[::-1]]\n",
    "        return top_n_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d416b48dc98312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T19:32:19.442844Z",
     "start_time": "2024-11-05T19:13:54.805566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 19687, Val size: 2188\n",
      "Loading exist preprocessed corpus: preprocessed_corpus.pkl\n",
      "Processing 'en' ...\n",
      "token tokenized_en.pkl already exsit，skip generation\n",
      "BM25 model 'bm25_model_en.joblib' already exist，skip generation\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(bm25_model_path):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBM25 model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbm25_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m already exist，skip generation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m     bm25_model \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbm25_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating BM25 model for  \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DIS\\lib\\site-packages\\joblib\\numpy_pickle.py:658\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[0;32m    656\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[1;32m--> 658\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DIS\\lib\\site-packages\\joblib\\numpy_pickle.py:577\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    575\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[0;32m    579\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[0;32m    583\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DIS\\lib\\pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1212\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DIS\\lib\\pickle.py:1239\u001b[0m, in \u001b[0;36m_Unpickler.load_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame_size \u001b[38;5;241m>\u001b[39m sys\u001b[38;5;241m.\u001b[39mmaxsize:\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe size > sys.maxsize: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m frame_size)\n\u001b[1;32m-> 1239\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unframer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DIS\\lib\\pickle.py:317\u001b[0m, in \u001b[0;36m_Unframer.load_frame\u001b[1;34m(self, frame_size)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_frame \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_frame\u001b[38;5;241m.\u001b[39mread() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[0;32m    316\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeginning of a new frame before end of current frame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_frame \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_size\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus_df = load_corpus('corpus.json')\n",
    "train_df, test_df = load_data('train.csv', 'test.csv')\n",
    "\n",
    "# split\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}\")\n",
    "\n",
    "# generate or load preprocessed corpus\n",
    "preprocessed_corpus_path = 'preprocessed_corpus.pkl'\n",
    "if os.path.exists(preprocessed_corpus_path):\n",
    "    print(f\"Loading exist preprocessed corpus: {preprocessed_corpus_path}\")\n",
    "    preprocessed_corpus = load_pickle(preprocessed_corpus_path)\n",
    "else:\n",
    "    document_texts = corpus_df['text'].tolist()\n",
    "    document_langs = corpus_df['lang'].tolist()\n",
    "    preprocessed_corpus = preprocess_texts(document_texts, document_langs)\n",
    "    save_pickle(preprocessed_corpus, preprocessed_corpus_path)\n",
    "    print(f\"Preprocessed corpus saved '{preprocessed_corpus_path}'。\")\n",
    "\n",
    "# process each lang\n",
    "lang_to_doc_indices = defaultdict(list)\n",
    "document_langs = corpus_df['lang'].tolist()\n",
    "document_ids = corpus_df['docid'].tolist()\n",
    "for idx, lang in enumerate(document_langs):\n",
    "    lang_to_doc_indices[lang].append(idx)\n",
    "\n",
    "bm25_models = {}\n",
    "doc_id_maps = {}\n",
    "\n",
    "for lang, indices in lang_to_doc_indices.items():\n",
    "    print(f\"Processing '{lang}' ...\")\n",
    "    lang_texts = [preprocessed_corpus[idx] for idx in indices]\n",
    "    lang_docids = [document_ids[idx] for idx in indices]\n",
    "    \n",
    "    # token\n",
    "    tokenized_lang_path = f'tokenized_{lang}.pkl'\n",
    "    if os.path.exists(tokenized_lang_path):\n",
    "        print(f\"token {tokenized_lang_path} already exsit，skip generation\")\n",
    "        lang_tokenized_corpus = load_pickle(tokenized_lang_path)\n",
    "    else:\n",
    "        print(f\"Generating tokens for '{lang}' ...\")\n",
    "        if lang == 'ko':\n",
    "            lang_tokenized_corpus = [okt.morphs(text) for text in tqdm(lang_texts, desc=f\"tokenizaion {lang}\")]\n",
    "        else:\n",
    "            lang_tokenized_corpus = [text.split() for text in lang_texts]\n",
    "        save_pickle(lang_tokenized_corpus, tokenized_lang_path)\n",
    "        print(f\"Tokens saved to '{tokenized_lang_path}'。\")\n",
    "    \n",
    "    # model\n",
    "    bm25_model_path = f'bm25_model_{lang}.joblib'\n",
    "    if os.path.exists(bm25_model_path):\n",
    "        print(f\"BM25 model '{bm25_model_path}' already exist，skip generation\")\n",
    "        bm25_model = joblib.load(bm25_model_path)\n",
    "    else:\n",
    "        print(f\"Generating BM25 model for  '{lang}' ...\")\n",
    "        bm25_model = BM25(lang_tokenized_corpus, k1=1.5, b=0.75)\n",
    "        joblib.dump(bm25_model, bm25_model_path)\n",
    "        print(f\"BM25 model saved at '{bm25_model_path}'。\")\n",
    "    \n",
    "    bm25_models[lang] = bm25_model\n",
    "    doc_id_maps[lang] = lang_docids\n",
    "    \n",
    "    del lang_tokenized_corpus, bm25_model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f467dcf79899b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recall_at_k(bm25_models, doc_id_maps, val_data, k=10):\n",
    "    recall_count = 0\n",
    "    total = len(val_data)\n",
    "    for idx, row in val_data.iterrows():\n",
    "        query = row['query']\n",
    "        lang = row['lang']\n",
    "        pos_doc = row['positive_docs']\n",
    "        if lang not in bm25_models:\n",
    "            continue\n",
    "        bm25_model = bm25_models[lang]\n",
    "        doc_ids = doc_id_maps[lang]\n",
    "        \n",
    "        query_clean = preprocess_text(query, lang)\n",
    "        if lang == 'ko':\n",
    "            tokenized_query = okt.morphs(query_clean)\n",
    "        else:\n",
    "            tokenized_query = query_clean.split()\n",
    "        \n",
    "        top_n_indices = bm25_model.retrieve_top_n(tokenized_query, n=k)\n",
    "        retrieved_doc_ids = [doc_ids[i] for i in top_n_indices]\n",
    "        \n",
    "        if pos_doc in retrieved_doc_ids:\n",
    "            recall_count += 1\n",
    "    \n",
    "    recall = recall_count / total if total > 0 else 0\n",
    "    return recall\n",
    "\n",
    "print(\"Computing Val Recall@10...\")\n",
    "recall_at_10 = evaluate_recall_at_k(bm25_models, doc_id_maps, val_data, k=10)\n",
    "print(f\"Val Recall@10: {recall_at_10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458bd8ea5e606b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_test_queries(bm25_models, doc_id_maps, test_df, k=10):\n",
    "    retrieved_docs = []\n",
    "    for idx, row in test_df.iterrows():\n",
    "        query_id = row['query_id']\n",
    "        query = row['query']\n",
    "        lang = row['lang']\n",
    "        if lang not in bm25_models:\n",
    "            retrieved_docs.append([])\n",
    "            continue\n",
    "        bm25_model = bm25_models[lang]\n",
    "        doc_ids = doc_id_maps[lang]\n",
    "        \n",
    "        query_clean = preprocess_text(query, lang)\n",
    "        if lang == 'ko':\n",
    "            tokenized_query = okt.morphs(query_clean)\n",
    "        else:\n",
    "            tokenized_query = query_clean.split()\n",
    "        \n",
    "        top_n_indices = bm25_model.retrieve_top_n(tokenized_query, n=k)\n",
    "        retrieved_doc_ids = [doc_ids[i] for i in top_n_indices]\n",
    "        retrieved_docs.append(retrieved_doc_ids)\n",
    "    return retrieved_docs\n",
    "\n",
    "print(\"Retrive on test queries...\")\n",
    "retrieved_docs_test = retrieve_test_queries(bm25_models, doc_id_maps, test_df, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca22da07ae36618",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    'id': np.arange(len(test_df)),\n",
    "    'docids': retrieved_docs_test\n",
    "})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"'submission.csv' done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
